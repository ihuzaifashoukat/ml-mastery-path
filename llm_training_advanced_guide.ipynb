{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc7bc04",
   "metadata": {},
   "source": [
    "# ðŸ§  Advanced LLM Training & Fine-Tuning Guide\n",
    "\n",
    "## From Pre-trained Models to Custom AI Assistants\n",
    "\n",
    "This notebook covers **everything you need to know** about training and fine-tuning Large Language Models (LLMs):\n",
    "\n",
    "### What You'll Learn:\n",
    "1. **LLM Architecture** - Transformers, Attention, Tokenization\n",
    "2. **Dataset Preparation** - Creating training data for LLMs\n",
    "3. **Fine-Tuning Strategies** - Full fine-tuning vs Parameter-Efficient methods\n",
    "4. **LoRA & QLoRA** - Train billion-parameter models on consumer GPUs\n",
    "5. **Training Loop** - Custom training with gradient accumulation\n",
    "6. **Evaluation** - Perplexity, BLEU, ROUGE metrics\n",
    "7. **Inference Optimization** - Quantization and deployment\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ¯ Understanding LLMs\n",
    "\n",
    "### What is an LLM?\n",
    "A **Large Language Model** is a neural network trained on massive text data to:\n",
    "- Predict the next token in a sequence\n",
    "- Understand and generate human language\n",
    "- Follow instructions and complete tasks\n",
    "\n",
    "### Key Architecture: Transformers\n",
    "\n",
    "```\n",
    "Input: \"The cat sat on the\"\n",
    "         â†“\n",
    "    [Tokenization]\n",
    "         â†“\n",
    "    [Embedding Layer] â†’ Convert tokens to vectors\n",
    "         â†“\n",
    "    [Transformer Blocks] Ã— N\n",
    "        â”œâ”€â”€ Multi-Head Self-Attention\n",
    "        â”œâ”€â”€ Feed-Forward Network\n",
    "        â””â”€â”€ Layer Normalization\n",
    "         â†“\n",
    "    [Output Layer] â†’ Probability distribution over vocabulary\n",
    "         â†“\n",
    "Output: \"mat\" (highest probability next token)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187e0310",
   "metadata": {},
   "source": [
    "## ðŸ“¦ Section 1: Install Required Libraries\n",
    "\n",
    "We'll use the **Hugging Face** ecosystem - the industry standard for LLM work:\n",
    "- `transformers` - Pre-trained models and training utilities\n",
    "- `datasets` - Efficient data loading and processing\n",
    "- `peft` - Parameter-Efficient Fine-Tuning (LoRA, etc.)\n",
    "- `bitsandbytes` - Quantization for memory efficiency\n",
    "- `accelerate` - Distributed training and mixed precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc7456c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (uncomment to run)\n",
    "# !pip install transformers datasets accelerate peft bitsandbytes trl sentencepiece\n",
    "# !pip install torch torchvision torchaudio  # If not already installed\n",
    "\n",
    "# Core imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.auto import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸ–¥ï¸ Using device: {device}\")\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"   GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "else:\n",
    "    print(\"   âš ï¸ No GPU detected. Training will be slow on CPU.\")\n",
    "    print(\"   Consider using Google Colab or a cloud GPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3e0eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hugging Face imports\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling,\n",
    "    DataCollatorWithPadding,\n",
    "    BitsAndBytesConfig,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    pipeline\n",
    ")\n",
    "from datasets import load_dataset, Dataset as HFDataset\n",
    "from peft import (\n",
    "    LoraConfig,\n",
    "    get_peft_model,\n",
    "    prepare_model_for_kbit_training,\n",
    "    TaskType\n",
    ")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")\n",
    "print(\"\\nðŸ“š Key Libraries:\")\n",
    "print(\"   - transformers: For loading and training models\")\n",
    "print(\"   - datasets: For efficient data handling\")\n",
    "print(\"   - peft: For LoRA and parameter-efficient training\")\n",
    "print(\"   - bitsandbytes: For quantization (4-bit, 8-bit)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae055e04",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”¤ Section 2: Understanding Tokenization\n",
    "\n",
    "### Why Tokenization?\n",
    "LLMs don't see text - they see **numbers**. Tokenization converts text to token IDs.\n",
    "\n",
    "### Types of Tokenization:\n",
    "| Method | Example | Pros | Cons |\n",
    "|--------|---------|------|------|\n",
    "| **Word-level** | \"Hello world\" â†’ [1, 2] | Simple | Huge vocabulary, OOV issues |\n",
    "| **Character-level** | \"Hi\" â†’ [8, 9] | Small vocab | Long sequences |\n",
    "| **Subword (BPE)** | \"playing\" â†’ [\"play\", \"ing\"] | Balanced | Requires training |\n",
    "\n",
    "Modern LLMs use **Byte-Pair Encoding (BPE)** or **SentencePiece**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b939c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a tokenizer from a pre-trained model\n",
    "model_name = \"gpt2\"  # Small model for demonstration\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# GPT-2 doesn't have a pad token by default\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"ðŸ”¤ TOKENIZATION DEEP DIVE\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Model: {model_name}\")\n",
    "print(f\"Vocabulary size: {tokenizer.vocab_size:,}\")\n",
    "print(f\"Max sequence length: {tokenizer.model_max_length:,}\")\n",
    "\n",
    "# Example tokenization\n",
    "text = \"Machine learning is transforming the world of artificial intelligence!\"\n",
    "\n",
    "# Tokenize\n",
    "tokens = tokenizer.tokenize(text)\n",
    "token_ids = tokenizer.encode(text)\n",
    "\n",
    "print(f\"\\nðŸ“ Original text: '{text}'\")\n",
    "print(f\"\\nðŸ”¢ Tokens ({len(tokens)}): {tokens}\")\n",
    "print(f\"\\nðŸ†” Token IDs: {token_ids}\")\n",
    "\n",
    "# Decode back\n",
    "decoded = tokenizer.decode(token_ids)\n",
    "print(f\"\\nðŸ”„ Decoded: '{decoded}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ca1d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tokenization\n",
    "def visualize_tokens(text, tokenizer):\n",
    "    \"\"\"Visualize how text is split into tokens\"\"\"\n",
    "    tokens = tokenizer.tokenize(text)\n",
    "    token_ids = tokenizer.encode(text, add_special_tokens=False)\n",
    "    \n",
    "    print(\"Token Breakdown:\")\n",
    "    print(\"-\" * 50)\n",
    "    for token, tid in zip(tokens, token_ids):\n",
    "        # Clean token representation (Ä  in GPT-2 means space before)\n",
    "        clean_token = token.replace('Ä ', 'â–')  # Show space marker\n",
    "        print(f\"  '{clean_token:15}' â†’ ID: {tid:6}\")\n",
    "\n",
    "# Examples\n",
    "examples = [\n",
    "    \"Hello, how are you?\",\n",
    "    \"The quick brown fox jumps\",\n",
    "    \"TensorFlow and PyTorch are frameworks\",\n",
    "    \"I'm learning about LLMs!\",\n",
    "    \"ðŸš€ Emojis work too! ðŸŽ‰\"\n",
    "]\n",
    "\n",
    "for text in examples:\n",
    "    print(f\"\\nðŸ“ Text: '{text}'\")\n",
    "    visualize_tokens(text, tokenizer)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7383688",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ—ï¸ Section 3: Building a Transformer from Scratch\n",
    "\n",
    "Let's build a **mini GPT** to understand the architecture deeply!\n",
    "\n",
    "### Self-Attention Mechanism\n",
    "\n",
    "The core of transformers is **self-attention**:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ = Query (what am I looking for?)\n",
    "- $K$ = Key (what do I contain?)\n",
    "- $V$ = Value (what information do I have?)\n",
    "- $d_k$ = dimension of keys (scaling factor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f14374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Mini GPT from scratch!\n",
    "\n",
    "class SelfAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Head Self-Attention mechanism.\n",
    "    This is the CORE of all transformer models!\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "        assert embed_dim % num_heads == 0, \"embed_dim must be divisible by num_heads\"\n",
    "        \n",
    "        # Linear projections for Q, K, V\n",
    "        self.q_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.k_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.v_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        self.out_proj = nn.Linear(embed_dim, embed_dim)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.scale = self.head_dim ** -0.5  # 1/sqrt(d_k)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        batch_size, seq_len, embed_dim = x.shape\n",
    "        \n",
    "        # Project to Q, K, V\n",
    "        Q = self.q_proj(x)  # (batch, seq, embed)\n",
    "        K = self.k_proj(x)\n",
    "        V = self.v_proj(x)\n",
    "        \n",
    "        # Reshape for multi-head attention\n",
    "        # (batch, seq, embed) -> (batch, num_heads, seq, head_dim)\n",
    "        Q = Q.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        K = K.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        V = V.view(batch_size, seq_len, self.num_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # Compute attention scores: (Q @ K^T) / sqrt(d_k)\n",
    "        attn_scores = torch.matmul(Q, K.transpose(-2, -1)) * self.scale\n",
    "        \n",
    "        # Apply causal mask (for autoregressive generation)\n",
    "        if mask is not None:\n",
    "            attn_scores = attn_scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax and dropout\n",
    "        attn_weights = F.softmax(attn_scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Apply attention to values\n",
    "        output = torch.matmul(attn_weights, V)\n",
    "        \n",
    "        # Reshape back: (batch, num_heads, seq, head_dim) -> (batch, seq, embed)\n",
    "        output = output.transpose(1, 2).contiguous().view(batch_size, seq_len, embed_dim)\n",
    "        \n",
    "        return self.out_proj(output)\n",
    "\n",
    "print(\"âœ… Self-Attention module created!\")\n",
    "print(\"\\nðŸ’¡ Key insight: Attention allows each token to 'look at' all other tokens\")\n",
    "print(\"   and decide which ones are most relevant for prediction.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb57223",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    A single Transformer block: Attention + Feed-Forward + Residuals + LayerNorm\n",
    "    \"\"\"\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Layer normalization (Pre-LN architecture, used in GPT-2+)\n",
    "        self.ln1 = nn.LayerNorm(embed_dim)\n",
    "        self.ln2 = nn.LayerNorm(embed_dim)\n",
    "        \n",
    "        # Self-attention\n",
    "        self.attention = SelfAttention(embed_dim, num_heads, dropout)\n",
    "        \n",
    "        # Feed-forward network (MLP)\n",
    "        self.ffn = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.GELU(),  # GPT uses GELU activation\n",
    "            nn.Linear(ff_dim, embed_dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "    \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm + Attention + Residual\n",
    "        x = x + self.dropout(self.attention(self.ln1(x), mask))\n",
    "        \n",
    "        # Pre-norm + FFN + Residual\n",
    "        x = x + self.dropout(self.ffn(self.ln2(x)))\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "class MiniGPT(nn.Module):\n",
    "    \"\"\"\n",
    "    A minimal GPT-style language model.\n",
    "    This is the architecture used by GPT-2, GPT-3, and most modern LLMs!\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, num_layers, max_seq_len, ff_dim=None, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embed_dim = embed_dim\n",
    "        ff_dim = ff_dim or 4 * embed_dim  # GPT uses 4x expansion\n",
    "        \n",
    "        # Token and position embeddings\n",
    "        self.token_embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embedding = nn.Embedding(max_seq_len, embed_dim)\n",
    "        \n",
    "        # Stack of transformer blocks\n",
    "        self.blocks = nn.ModuleList([\n",
    "            TransformerBlock(embed_dim, num_heads, ff_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Final layer norm and output projection\n",
    "        self.ln_final = nn.LayerNorm(embed_dim)\n",
    "        self.lm_head = nn.Linear(embed_dim, vocab_size, bias=False)\n",
    "        \n",
    "        # Weight tying (optional but common)\n",
    "        self.token_embedding.weight = self.lm_head.weight\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        \n",
    "        # Create causal mask\n",
    "        self.register_buffer(\n",
    "            \"causal_mask\",\n",
    "            torch.tril(torch.ones(max_seq_len, max_seq_len)).view(1, 1, max_seq_len, max_seq_len)\n",
    "        )\n",
    "    \n",
    "    def forward(self, input_ids, labels=None):\n",
    "        batch_size, seq_len = input_ids.shape\n",
    "        \n",
    "        # Get embeddings\n",
    "        positions = torch.arange(seq_len, device=input_ids.device).unsqueeze(0)\n",
    "        x = self.token_embedding(input_ids) + self.position_embedding(positions)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Get causal mask for current sequence length\n",
    "        mask = self.causal_mask[:, :, :seq_len, :seq_len]\n",
    "        \n",
    "        # Pass through transformer blocks\n",
    "        for block in self.blocks:\n",
    "            x = block(x, mask)\n",
    "        \n",
    "        # Final layer norm and projection to vocabulary\n",
    "        x = self.ln_final(x)\n",
    "        logits = self.lm_head(x)  # (batch, seq, vocab_size)\n",
    "        \n",
    "        # Calculate loss if labels provided\n",
    "        loss = None\n",
    "        if labels is not None:\n",
    "            # Shift logits and labels for next-token prediction\n",
    "            shift_logits = logits[:, :-1, :].contiguous()\n",
    "            shift_labels = labels[:, 1:].contiguous()\n",
    "            loss = F.cross_entropy(\n",
    "                shift_logits.view(-1, shift_logits.size(-1)),\n",
    "                shift_labels.view(-1)\n",
    "            )\n",
    "        \n",
    "        return {\"logits\": logits, \"loss\": loss}\n",
    "\n",
    "print(\"âœ… MiniGPT architecture created!\")\n",
    "print(\"\\nðŸ“Š Architecture Summary:\")\n",
    "print(\"   1. Token Embedding: Convert token IDs to vectors\")\n",
    "print(\"   2. Position Embedding: Add positional information\")\n",
    "print(\"   3. N Ã— Transformer Blocks: Attention + FFN\")\n",
    "print(\"   4. LM Head: Project back to vocabulary for prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbffa85f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test our Mini GPT\n",
    "config = {\n",
    "    \"vocab_size\": tokenizer.vocab_size,\n",
    "    \"embed_dim\": 256,\n",
    "    \"num_heads\": 8,\n",
    "    \"num_layers\": 4,\n",
    "    \"max_seq_len\": 512,\n",
    "    \"dropout\": 0.1\n",
    "}\n",
    "\n",
    "model = MiniGPT(**config).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"ðŸ¤– MINI GPT MODEL\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Configuration:\")\n",
    "for k, v in config.items():\n",
    "    print(f\"   {k}: {v}\")\n",
    "print(f\"\\nðŸ“Š Parameters:\")\n",
    "print(f\"   Total: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"   Trainable: {trainable_params:,}\")\n",
    "\n",
    "# Test forward pass\n",
    "test_input = torch.randint(0, config[\"vocab_size\"], (2, 64)).to(device)\n",
    "with torch.no_grad():\n",
    "    output = model(test_input)\n",
    "print(f\"\\nâœ… Forward pass successful!\")\n",
    "print(f\"   Input shape: {test_input.shape}\")\n",
    "print(f\"   Output logits shape: {output['logits'].shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae1bb048",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“š Section 4: Dataset Preparation for LLM Training\n",
    "\n",
    "### Types of Training Data:\n",
    "\n",
    "| Format | Use Case | Example |\n",
    "|--------|----------|---------|\n",
    "| **Raw Text** | Pre-training | Books, Wikipedia, Web pages |\n",
    "| **Instruction-Following** | Fine-tuning | `{\"instruction\": ..., \"response\": ...}` |\n",
    "| **Conversation** | Chat models | `[{\"role\": \"user\"}, {\"role\": \"assistant\"}]` |\n",
    "| **Preference** | RLHF/DPO | `{\"chosen\": ..., \"rejected\": ...}` |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9498b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample dataset from Hugging Face\n",
    "print(\"ðŸ“š LOADING TRAINING DATASET\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Load a small instruction-following dataset\n",
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")  # First 1000 examples\n",
    "\n",
    "print(f\"Dataset size: {len(dataset)}\")\n",
    "print(f\"Features: {dataset.features}\")\n",
    "print(f\"\\nðŸ“ Sample entry:\")\n",
    "sample = dataset[0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, str) and len(value) > 200:\n",
    "        print(f\"   {key}: {value[:200]}...\")\n",
    "    else:\n",
    "        print(f\"   {key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19df2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instruction prompt templates\n",
    "def format_instruction(example):\n",
    "    \"\"\"\n",
    "    Format instruction-following data into a prompt.\n",
    "    This is how we teach the model to follow instructions!\n",
    "    \"\"\"\n",
    "    instruction = example[\"instruction\"]\n",
    "    input_text = example.get(\"input\", \"\")\n",
    "    output = example[\"output\"]\n",
    "    \n",
    "    if input_text:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Input:\n",
    "{input_text}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    else:\n",
    "        prompt = f\"\"\"### Instruction:\n",
    "{instruction}\n",
    "\n",
    "### Response:\n",
    "{output}\"\"\"\n",
    "    \n",
    "    return {\"text\": prompt}\n",
    "\n",
    "# Apply formatting\n",
    "formatted_dataset = dataset.map(format_instruction)\n",
    "\n",
    "print(\"ðŸ“ FORMATTED PROMPT EXAMPLE:\")\n",
    "print(\"=\"*60)\n",
    "print(formatted_dataset[0][\"text\"][:500])\n",
    "print(\"...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6549bfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the dataset for training\n",
    "def tokenize_function(examples):\n",
    "    \"\"\"Tokenize texts and create labels for causal LM\"\"\"\n",
    "    tokenized = tokenizer(\n",
    "        examples[\"text\"],\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        padding=\"max_length\",\n",
    "        return_tensors=None  # Return lists for dataset mapping\n",
    "    )\n",
    "    # For causal LM, labels = input_ids (shifted internally)\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
    "    return tokenized\n",
    "\n",
    "# Tokenize\n",
    "tokenized_dataset = formatted_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=formatted_dataset.column_names\n",
    ")\n",
    "\n",
    "# Split into train/validation\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"ðŸ”¢ TOKENIZED DATASET\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Training samples: {len(train_dataset)}\")\n",
    "print(f\"Validation samples: {len(eval_dataset)}\")\n",
    "print(f\"Features: {train_dataset.features}\")\n",
    "print(f\"\\nSample tokenized entry:\")\n",
    "print(f\"   input_ids length: {len(train_dataset[0]['input_ids'])}\")\n",
    "print(f\"   First 10 tokens: {train_dataset[0]['input_ids'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa9b132",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ¯ Section 5: Fine-Tuning Strategies\n",
    "\n",
    "### Full Fine-Tuning vs Parameter-Efficient Fine-Tuning\n",
    "\n",
    "| Method | Parameters Updated | Memory | Speed | Best For |\n",
    "|--------|-------------------|--------|-------|----------|\n",
    "| **Full Fine-Tuning** | All | Very High | Slow | Small models, large datasets |\n",
    "| **LoRA** | ~0.1-1% | Low | Fast | Most cases |\n",
    "| **QLoRA** | ~0.1-1% + Quantization | Very Low | Medium | Limited GPU memory |\n",
    "| **Prefix Tuning** | Virtual tokens | Low | Fast | Specific tasks |\n",
    "\n",
    "### LoRA (Low-Rank Adaptation)\n",
    "\n",
    "Instead of updating all weights $W$, we learn low-rank matrices $A$ and $B$:\n",
    "\n",
    "$$W_{new} = W_{frozen} + \\alpha \\cdot BA$$\n",
    "\n",
    "Where:\n",
    "- $W \\in \\mathbb{R}^{d \\times k}$ (original weight)\n",
    "- $B \\in \\mathbb{R}^{d \\times r}$ (down projection)\n",
    "- $A \\in \\mathbb{R}^{r \\times k}$ (up projection)\n",
    "- $r \\ll \\min(d, k)$ (rank, typically 8-64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86929a43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a pre-trained model for fine-tuning\n",
    "print(\"ðŸ¤– LOADING PRE-TRAINED MODEL\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Using GPT-2 small for demonstration (can scale to larger models)\n",
    "model_name = \"gpt2\"\n",
    "\n",
    "# Load with optional quantization for memory efficiency\n",
    "use_quantization = False  # Set to True if you have limited GPU memory\n",
    "\n",
    "if use_quantization and torch.cuda.is_available():\n",
    "    # 4-bit quantization config (QLoRA)\n",
    "    bnb_config = BitsAndBytesConfig(\n",
    "        load_in_4bit=True,\n",
    "        bnb_4bit_quant_type=\"nf4\",\n",
    "        bnb_4bit_compute_dtype=torch.float16,\n",
    "        bnb_4bit_use_double_quant=True\n",
    "    )\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        quantization_config=bnb_config,\n",
    "        device_map=\"auto\"\n",
    "    )\n",
    "    print(\"âœ… Model loaded with 4-bit quantization (QLoRA)\")\n",
    "else:\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "    base_model = base_model.to(device)\n",
    "    print(\"âœ… Model loaded in full precision\")\n",
    "\n",
    "# Model info\n",
    "total_params = sum(p.numel() for p in base_model.parameters())\n",
    "print(f\"\\nðŸ“Š Model: {model_name}\")\n",
    "print(f\"   Total parameters: {total_params:,} ({total_params/1e6:.2f}M)\")\n",
    "print(f\"   Model size: ~{total_params * 4 / 1e9:.2f} GB (FP32)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa0b4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LoRA for efficient fine-tuning\n",
    "print(\"ðŸ”§ APPLYING LoRA\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    r=16,                        # Rank of the update matrices\n",
    "    lora_alpha=32,               # Scaling factor\n",
    "    target_modules=[\"c_attn\", \"c_proj\"],  # Which layers to apply LoRA to\n",
    "    lora_dropout=0.05,           # Dropout for LoRA layers\n",
    "    bias=\"none\",                 # Don't train bias terms\n",
    "    task_type=TaskType.CAUSAL_LM # Task type\n",
    ")\n",
    "\n",
    "# Prepare model for LoRA\n",
    "if use_quantization:\n",
    "    base_model = prepare_model_for_kbit_training(base_model)\n",
    "\n",
    "# Apply LoRA\n",
    "model_lora = get_peft_model(base_model, lora_config)\n",
    "\n",
    "# Compare parameters\n",
    "trainable_params = sum(p.numel() for p in model_lora.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in model_lora.parameters())\n",
    "\n",
    "print(f\"\\nðŸ“Š LoRA Statistics:\")\n",
    "print(f\"   Total parameters: {total_params:,}\")\n",
    "print(f\"   Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"   Trainable %: {100 * trainable_params / total_params:.4f}%\")\n",
    "print(f\"\\nðŸ’¡ We're only training {trainable_params/1e6:.2f}M parameters instead of {total_params/1e6:.2f}M!\")\n",
    "print(\"   This makes fine-tuning possible on consumer GPUs!\")\n",
    "\n",
    "# Show LoRA modules\n",
    "print(\"\\nðŸŽ¯ LoRA applied to these modules:\")\n",
    "model_lora.print_trainable_parameters()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc14ce9d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ”„ Section 6: The Training Loop - Deep Dive\n",
    "\n",
    "Let's build a **complete training loop** from scratch to understand every step!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffca2455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Training Loop for LLMs\n",
    "class LLMTrainer:\n",
    "    \"\"\"\n",
    "    A complete training loop for LLMs.\n",
    "    Shows exactly what happens during training!\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, \n",
    "        model, \n",
    "        tokenizer,\n",
    "        train_dataset,\n",
    "        eval_dataset=None,\n",
    "        batch_size=4,\n",
    "        learning_rate=2e-5,\n",
    "        num_epochs=3,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=100,\n",
    "        max_grad_norm=1.0,\n",
    "        device=\"cuda\"\n",
    "    ):\n",
    "        self.model = model.to(device)\n",
    "        self.tokenizer = tokenizer\n",
    "        self.device = device\n",
    "        self.batch_size = batch_size\n",
    "        self.gradient_accumulation_steps = gradient_accumulation_steps\n",
    "        self.max_grad_norm = max_grad_norm\n",
    "        self.num_epochs = num_epochs\n",
    "        \n",
    "        # Data collator for language modeling\n",
    "        self.data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=tokenizer,\n",
    "            mlm=False  # Causal LM, not Masked LM\n",
    "        )\n",
    "        \n",
    "        # Create dataloaders\n",
    "        self.train_dataloader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.data_collator\n",
    "        )\n",
    "        \n",
    "        self.eval_dataloader = None\n",
    "        if eval_dataset:\n",
    "            self.eval_dataloader = DataLoader(\n",
    "                eval_dataset,\n",
    "                batch_size=batch_size,\n",
    "                collate_fn=self.data_collator\n",
    "            )\n",
    "        \n",
    "        # Calculate total steps\n",
    "        total_steps = len(self.train_dataloader) * num_epochs // gradient_accumulation_steps\n",
    "        \n",
    "        # Optimizer - AdamW is standard for transformers\n",
    "        self.optimizer = torch.optim.AdamW(\n",
    "            model.parameters(),\n",
    "            lr=learning_rate,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8,\n",
    "            weight_decay=0.01\n",
    "        )\n",
    "        \n",
    "        # Learning rate scheduler with warmup\n",
    "        self.scheduler = get_linear_schedule_with_warmup(\n",
    "            self.optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "        \n",
    "        # Training history\n",
    "        self.history = {\n",
    "            \"train_loss\": [],\n",
    "            \"eval_loss\": [],\n",
    "            \"learning_rate\": []\n",
    "        }\n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"Main training loop\"\"\"\n",
    "        print(\"ðŸš€ STARTING TRAINING\")\n",
    "        print(\"=\"*60)\n",
    "        print(f\"   Epochs: {self.num_epochs}\")\n",
    "        print(f\"   Batch size: {self.batch_size}\")\n",
    "        print(f\"   Gradient accumulation: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"   Effective batch size: {self.batch_size * self.gradient_accumulation_steps}\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        global_step = 0\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            progress_bar = tqdm(self.train_dataloader, desc=f\"Epoch {epoch+1}/{self.num_epochs}\")\n",
    "            \n",
    "            for step, batch in enumerate(progress_bar):\n",
    "                # Move batch to device\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = self.model(**batch)\n",
    "                loss = outputs.loss\n",
    "                \n",
    "                # Scale loss for gradient accumulation\n",
    "                loss = loss / self.gradient_accumulation_steps\n",
    "                \n",
    "                # Backward pass\n",
    "                loss.backward()\n",
    "                \n",
    "                epoch_loss += loss.item() * self.gradient_accumulation_steps\n",
    "                \n",
    "                # Update weights every gradient_accumulation_steps\n",
    "                if (step + 1) % self.gradient_accumulation_steps == 0:\n",
    "                    # Gradient clipping (prevents exploding gradients)\n",
    "                    torch.nn.utils.clip_grad_norm_(\n",
    "                        self.model.parameters(), \n",
    "                        self.max_grad_norm\n",
    "                    )\n",
    "                    \n",
    "                    # Optimizer step\n",
    "                    self.optimizer.step()\n",
    "                    self.scheduler.step()\n",
    "                    self.optimizer.zero_grad()\n",
    "                    \n",
    "                    global_step += 1\n",
    "                    \n",
    "                    # Log learning rate\n",
    "                    current_lr = self.scheduler.get_last_lr()[0]\n",
    "                    self.history[\"learning_rate\"].append(current_lr)\n",
    "                \n",
    "                # Update progress bar\n",
    "                progress_bar.set_postfix({\n",
    "                    \"loss\": f\"{loss.item() * self.gradient_accumulation_steps:.4f}\",\n",
    "                    \"lr\": f\"{self.scheduler.get_last_lr()[0]:.2e}\"\n",
    "                })\n",
    "            \n",
    "            # Epoch metrics\n",
    "            avg_train_loss = epoch_loss / len(self.train_dataloader)\n",
    "            self.history[\"train_loss\"].append(avg_train_loss)\n",
    "            \n",
    "            print(f\"\\nðŸ“Š Epoch {epoch+1} Summary:\")\n",
    "            print(f\"   Train Loss: {avg_train_loss:.4f}\")\n",
    "            \n",
    "            # Evaluation\n",
    "            if self.eval_dataloader:\n",
    "                eval_loss = self.evaluate()\n",
    "                self.history[\"eval_loss\"].append(eval_loss)\n",
    "                print(f\"   Eval Loss: {eval_loss:.4f}\")\n",
    "                print(f\"   Perplexity: {np.exp(eval_loss):.2f}\")\n",
    "        \n",
    "        return self.history\n",
    "    \n",
    "    def evaluate(self):\n",
    "        \"\"\"Evaluation loop\"\"\"\n",
    "        self.model.eval()\n",
    "        total_loss = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for batch in self.eval_dataloader:\n",
    "                batch = {k: v.to(self.device) for k, v in batch.items()}\n",
    "                outputs = self.model(**batch)\n",
    "                total_loss += outputs.loss.item()\n",
    "        \n",
    "        return total_loss / len(self.eval_dataloader)\n",
    "\n",
    "print(\"âœ… LLMTrainer class created!\")\n",
    "print(\"\\nðŸ“‹ Key Training Concepts:\")\n",
    "print(\"   1. Gradient Accumulation: Simulate larger batches with limited memory\")\n",
    "print(\"   2. Warmup: Gradually increase learning rate at start\")\n",
    "print(\"   3. Gradient Clipping: Prevent exploding gradients\")\n",
    "print(\"   4. AdamW: Adam optimizer with proper weight decay\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df7628a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model (using Hugging Face Trainer for convenience)\n",
    "print(\"ðŸ‹ï¸ TRAINING WITH HUGGING FACE TRAINER\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./llm_output\",\n",
    "    num_train_epochs=1,                    # Reduce for demo\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    gradient_accumulation_steps=4,         # Effective batch = 2 * 4 = 8\n",
    "    warmup_steps=50,\n",
    "    learning_rate=2e-4,                    # Higher LR for LoRA\n",
    "    fp16=torch.cuda.is_available(),        # Mixed precision if GPU\n",
    "    logging_steps=10,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=50,\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    report_to=\"none\",                      # Disable wandb/tensorboard\n",
    "    optim=\"adamw_torch\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    gradient_checkpointing=False,          # Trade compute for memory\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = Trainer(\n",
    "    model=model_lora,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "print(\"ðŸ“‹ Training Configuration:\")\n",
    "print(f\"   Epochs: {training_args.num_train_epochs}\")\n",
    "print(f\"   Batch size: {training_args.per_device_train_batch_size}\")\n",
    "print(f\"   Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Effective batch: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
    "print(f\"   Learning rate: {training_args.learning_rate}\")\n",
    "print(f\"   Mixed precision (FP16): {training_args.fp16}\")\n",
    "print(\"\\nâ³ Training... (this may take a few minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fed0cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run training (uncomment to actually train)\n",
    "# train_result = trainer.train()\n",
    "\n",
    "# For demo purposes, let's show what the training output looks like\n",
    "print(\"ðŸ“Š EXAMPLE TRAINING OUTPUT:\")\n",
    "print(\"-\"*60)\n",
    "print(\"\"\"\n",
    "{'loss': 3.2451, 'learning_rate': 0.0002, 'epoch': 0.05}\n",
    "{'loss': 2.8923, 'learning_rate': 0.00019, 'epoch': 0.10}\n",
    "{'loss': 2.5674, 'learning_rate': 0.00018, 'epoch': 0.15}\n",
    "...\n",
    "{'eval_loss': 2.1234, 'eval_runtime': 5.23, 'epoch': 1.0}\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸ’¡ Training Tips:\")\n",
    "print(\"   1. Watch for loss decreasing over time\")\n",
    "print(\"   2. Eval loss should track train loss (not diverge)\")\n",
    "print(\"   3. If eval loss increases = overfitting!\")\n",
    "print(\"   4. Learning rate should decrease (cosine schedule)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0486e798",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“ Section 7: Evaluation Metrics for LLMs\n",
    "\n",
    "### Key Metrics:\n",
    "\n",
    "| Metric | Formula/Description | Use Case |\n",
    "|--------|---------------------|----------|\n",
    "| **Perplexity** | $PPL = e^{-\\frac{1}{N}\\sum \\log P(x_i)}$ | Language modeling quality |\n",
    "| **BLEU** | n-gram precision with brevity penalty | Translation, generation |\n",
    "| **ROUGE** | Recall-oriented n-gram overlap | Summarization |\n",
    "| **F1 Score** | Harmonic mean of P & R | Classification tasks |\n",
    "| **Exact Match** | Percentage of perfect matches | QA tasks |\n",
    "\n",
    "### Perplexity Explained\n",
    "\n",
    "Perplexity measures how \"surprised\" the model is by the text:\n",
    "- **Lower = Better** (model predicts well)\n",
    "- A perplexity of 10 means the model is as uncertain as randomly choosing from 10 equally likely options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b41120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate Perplexity\n",
    "def calculate_perplexity(model, dataloader, device):\n",
    "    \"\"\"\n",
    "    Calculate perplexity on a dataset.\n",
    "    Perplexity = exp(average cross-entropy loss)\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    total_tokens = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Calculating perplexity\"):\n",
    "            batch = {k: v.to(device) for k, v in batch.items()}\n",
    "            outputs = model(**batch)\n",
    "            \n",
    "            # Get loss (averaged over tokens)\n",
    "            loss = outputs.loss.item()\n",
    "            \n",
    "            # Count non-padding tokens\n",
    "            num_tokens = (batch[\"attention_mask\"].sum()).item()\n",
    "            \n",
    "            total_loss += loss * num_tokens\n",
    "            total_tokens += num_tokens\n",
    "    \n",
    "    avg_loss = total_loss / total_tokens\n",
    "    perplexity = np.exp(avg_loss)\n",
    "    \n",
    "    return perplexity, avg_loss\n",
    "\n",
    "# Example perplexity values for reference\n",
    "print(\"ðŸ“Š PERPLEXITY BENCHMARKS\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "Model Performance Examples:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Model                   â”‚ Perplexity  â”‚ Quality             â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Random baseline         â”‚ ~50,000     â”‚ Completely random   â”‚\n",
    "â”‚ N-gram model            â”‚ ~200-500    â”‚ Basic patterns      â”‚\n",
    "â”‚ GPT-2 Small (117M)      â”‚ ~35-40      â”‚ Good fluency        â”‚\n",
    "â”‚ GPT-2 Large (774M)      â”‚ ~20-25      â”‚ Very good           â”‚\n",
    "â”‚ GPT-3 (175B)            â”‚ ~15-20      â”‚ Excellent           â”‚\n",
    "â”‚ Fine-tuned on domain    â”‚ ~5-15       â”‚ Domain expert       â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ðŸ’¡ Lower perplexity = better language modeling\n",
    "   But doesn't always mean better task performance!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c7a7fd1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ­ Section 8: Text Generation & Inference\n",
    "\n",
    "Now let's generate text with our model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53dfa443",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Generation Strategies\n",
    "print(\"ðŸŽ­ TEXT GENERATION STRATEGIES\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "def generate_text(model, tokenizer, prompt, max_length=100, **kwargs):\n",
    "    \"\"\"Generate text with various decoding strategies\"\"\"\n",
    "    model.eval()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_length=max_length,\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            **kwargs\n",
    "        )\n",
    "    \n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# Load base GPT-2 for generation demo\n",
    "gen_model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(device)\n",
    "\n",
    "prompt = \"The future of artificial intelligence\"\n",
    "\n",
    "# Different generation strategies\n",
    "strategies = {\n",
    "    \"Greedy (deterministic)\": {\n",
    "        \"do_sample\": False\n",
    "    },\n",
    "    \"Temperature Sampling (creative)\": {\n",
    "        \"do_sample\": True,\n",
    "        \"temperature\": 0.8,\n",
    "        \"top_k\": 0\n",
    "    },\n",
    "    \"Top-K Sampling\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_k\": 50,\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    \"Top-P (Nucleus) Sampling\": {\n",
    "        \"do_sample\": True,\n",
    "        \"top_p\": 0.9,\n",
    "        \"temperature\": 0.7\n",
    "    },\n",
    "    \"Beam Search\": {\n",
    "        \"do_sample\": False,\n",
    "        \"num_beams\": 5,\n",
    "        \"early_stopping\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "print(f\"ðŸ“ Prompt: '{prompt}'\")\n",
    "print(\"-\"*60)\n",
    "\n",
    "for name, params in strategies.items():\n",
    "    print(f\"\\nðŸŽ¯ {name}:\")\n",
    "    output = generate_text(gen_model, tokenizer, prompt, max_length=80, **params)\n",
    "    print(f\"   {output}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b725aa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize generation strategies\n",
    "print(\"ðŸ“Š DECODING STRATEGY COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "print(\"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Strategy            â”‚ Parameters      â”‚ Best For                        â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Greedy              â”‚ -               â”‚ Deterministic tasks             â”‚\n",
    "â”‚ Temperature         â”‚ temp=0.1-2.0    â”‚ Controlling creativity          â”‚\n",
    "â”‚ Top-K               â”‚ k=10-100        â”‚ Limiting vocabulary             â”‚\n",
    "â”‚ Top-P (Nucleus)     â”‚ p=0.9-0.95      â”‚ Dynamic vocabulary cutoff       â”‚\n",
    "â”‚ Beam Search         â”‚ beams=3-10      â”‚ Translation, quality focus      â”‚\n",
    "â”‚ Contrastive         â”‚ penalty=1.0     â”‚ Reducing repetition             â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ðŸ’¡ Common combinations:\n",
    "   - Chatbots: Top-P=0.9, Temperature=0.7\n",
    "   - Creative writing: Temperature=0.9, Top-K=50\n",
    "   - Code generation: Temperature=0.2, Top-P=0.95\n",
    "   - Translation: Beam Search with 4-5 beams\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a95183b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ’¾ Section 9: Saving, Loading & Deploying Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8ada9ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving and Loading Models\n",
    "print(\"ðŸ’¾ SAVING & LOADING MODELS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save LoRA adapter (small!)\n",
    "lora_save_path = \"./lora_adapter\"\n",
    "# model_lora.save_pretrained(lora_save_path)  # Uncomment to save\n",
    "print(f\"âœ… LoRA adapter would be saved to: {lora_save_path}\")\n",
    "print(\"   Size: ~10-50 MB (vs GBs for full model)\")\n",
    "\n",
    "# Save tokenizer\n",
    "# tokenizer.save_pretrained(lora_save_path)\n",
    "\n",
    "# To load later:\n",
    "print(\"\"\"\n",
    "ðŸ“¥ Loading a fine-tuned model:\n",
    "\n",
    "from peft import PeftModel\n",
    "\n",
    "# Load base model\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "\n",
    "# Load LoRA adapter\n",
    "model = PeftModel.from_pretrained(base_model, \"./lora_adapter\")\n",
    "\n",
    "# For inference, merge LoRA weights into base model\n",
    "model = model.merge_and_unload()  # Optional: Makes inference faster\n",
    "\"\"\")\n",
    "\n",
    "# Merge and unload for deployment\n",
    "print(\"\\nðŸ”§ Merging LoRA weights for deployment:\")\n",
    "print(\"   merged_model = model.merge_and_unload()\")\n",
    "print(\"   This combines LoRA weights with base model for faster inference.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75243764",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸš€ Section 10: Advanced Topics - RLHF & DPO\n",
    "\n",
    "### Reinforcement Learning from Human Feedback (RLHF)\n",
    "\n",
    "The technique that made ChatGPT so good at following instructions:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                           RLHF PIPELINE                                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                        â”‚\n",
    "â”‚  1. PRE-TRAINING (Base LLM)                                            â”‚\n",
    "â”‚     â””â”€> Train on massive text data (next token prediction)             â”‚\n",
    "â”‚                                                                        â”‚\n",
    "â”‚  2. SUPERVISED FINE-TUNING (SFT)                                       â”‚\n",
    "â”‚     â””â”€> Train on instruction-response pairs                            â”‚\n",
    "â”‚                                                                        â”‚\n",
    "â”‚  3. REWARD MODEL TRAINING                                              â”‚\n",
    "â”‚     â”œâ”€> Collect human comparisons: \"Which response is better?\"         â”‚\n",
    "â”‚     â””â”€> Train a model to predict human preferences                     â”‚\n",
    "â”‚                                                                        â”‚\n",
    "â”‚  4. REINFORCEMENT LEARNING (PPO)                                       â”‚\n",
    "â”‚     â””â”€> Optimize policy to maximize reward while staying close         â”‚\n",
    "â”‚         to SFT model (KL divergence penalty)                           â”‚\n",
    "â”‚                                                                        â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "### Direct Preference Optimization (DPO)\n",
    "\n",
    "A simpler alternative to RLHF - no reward model needed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1836b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DPO Training Example (using trl library)\n",
    "print(\"ðŸŽ¯ DIRECT PREFERENCE OPTIMIZATION (DPO)\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# DPO loss function (simplified)\n",
    "def dpo_loss(policy_chosen_logps, policy_rejected_logps, \n",
    "             ref_chosen_logps, ref_rejected_logps, beta=0.1):\n",
    "    \"\"\"\n",
    "    DPO Loss = -log(Ïƒ(Î² * (log(Ï€(y_w|x)/Ï€_ref(y_w|x)) - log(Ï€(y_l|x)/Ï€_ref(y_l|x)))))\n",
    "    \n",
    "    Where:\n",
    "    - y_w = chosen (winning) response\n",
    "    - y_l = rejected (losing) response\n",
    "    - Ï€ = policy model\n",
    "    - Ï€_ref = reference model (frozen)\n",
    "    - Î² = temperature parameter\n",
    "    \"\"\"\n",
    "    chosen_rewards = beta * (policy_chosen_logps - ref_chosen_logps)\n",
    "    rejected_rewards = beta * (policy_rejected_logps - ref_rejected_logps)\n",
    "    \n",
    "    loss = -F.logsigmoid(chosen_rewards - rejected_rewards).mean()\n",
    "    \n",
    "    return loss\n",
    "\n",
    "print(\"\"\"\n",
    "ðŸ“Š DPO vs RLHF:\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Aspect              â”‚ RLHF            â”‚ DPO                       â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Reward Model        â”‚ Required        â”‚ Not needed                â”‚\n",
    "â”‚ Complexity          â”‚ High            â”‚ Low                       â”‚\n",
    "â”‚ Stability           â”‚ Can be unstable â”‚ More stable               â”‚\n",
    "â”‚ Compute             â”‚ Higher          â”‚ Lower                     â”‚\n",
    "â”‚ Performance         â”‚ SOTA            â”‚ Competitive               â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\n",
    "ðŸ’¡ DPO directly optimizes the policy using preference data,\n",
    "   bypassing the need for a separate reward model!\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09d0383d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸ“‹ Section 11: Complete Training Checklist\n",
    "\n",
    "### Before Training:\n",
    "- [ ] Choose base model appropriate for your task and compute budget\n",
    "- [ ] Prepare high-quality training data (garbage in = garbage out!)\n",
    "- [ ] Clean and deduplicate data\n",
    "- [ ] Split data: train/val/test\n",
    "- [ ] Set up proper prompt templates\n",
    "\n",
    "### During Training:\n",
    "- [ ] Monitor loss curves (train AND validation)\n",
    "- [ ] Watch for overfitting (val loss increasing)\n",
    "- [ ] Check GPU memory usage\n",
    "- [ ] Log metrics with Weights & Biases or TensorBoard\n",
    "- [ ] Save checkpoints regularly\n",
    "\n",
    "### After Training:\n",
    "- [ ] Evaluate on held-out test set\n",
    "- [ ] Qualitative evaluation (look at actual outputs!)\n",
    "- [ ] Compare with baseline model\n",
    "- [ ] Test for biases and safety issues\n",
    "- [ ] Save model and training config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf7de87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common hyperparameters and their typical ranges\n",
    "print(\"ðŸŽ›ï¸ HYPERPARAMETER GUIDE\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "hyperparams = \"\"\"\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚ Hyperparameter         â”‚ Typical Range   â”‚ Notes                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Learning Rate          â”‚ 1e-5 to 5e-4    â”‚ Lower for full FT, higher for   â”‚\n",
    "â”‚                        â”‚                 â”‚ LoRA. Start with 2e-5.          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Batch Size             â”‚ 8 to 128        â”‚ Larger = more stable, needs moreâ”‚\n",
    "â”‚ (effective)            â”‚                 â”‚ memory. Use grad accumulation.  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Epochs                 â”‚ 1 to 5          â”‚ LLMs often need only 1-3 epochs â”‚\n",
    "â”‚                        â”‚                 â”‚ Overfitting is easy!            â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Warmup Ratio           â”‚ 0.03 to 0.1     â”‚ Gradual LR increase at start    â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Weight Decay           â”‚ 0.01 to 0.1     â”‚ Regularization                  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ LoRA Rank (r)          â”‚ 8 to 64         â”‚ Higher = more capacity, slower  â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ LoRA Alpha             â”‚ 16 to 64        â”‚ Usually 2x rank                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚ Max Sequence Length    â”‚ 512 to 4096     â”‚ Longer = more memory            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "\"\"\"\n",
    "print(hyperparams)\n",
    "\n",
    "print(\"ðŸ’¡ Pro Tips:\")\n",
    "print(\"   1. Start with default values and tune one at a time\")\n",
    "print(\"   2. Use learning rate finder to find optimal LR\")\n",
    "print(\"   3. If loss doesn't decrease: LR too low or data issues\")\n",
    "print(\"   4. If loss spikes: LR too high, reduce by 10x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d049b1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ“ Summary: Complete LLM Training Pipeline\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    COMPLETE LLM TRAINING PIPELINE                          â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                            â”‚\n",
    "â”‚  ðŸ“¥ 1. DATA PREPARATION                                                    â”‚\n",
    "â”‚     â”œâ”€> Collect instruction-response pairs                                 â”‚\n",
    "â”‚     â”œâ”€> Format into prompt templates                                       â”‚\n",
    "â”‚     â”œâ”€> Tokenize with appropriate max_length                               â”‚\n",
    "â”‚     â””â”€> Split: Train (90%) / Validation (10%)                              â”‚\n",
    "â”‚                                                                            â”‚\n",
    "â”‚  ðŸ¤– 2. MODEL SETUP                                                         â”‚\n",
    "â”‚     â”œâ”€> Choose base model (GPT-2, LLaMA, Mistral, etc.)                   â”‚\n",
    "â”‚     â”œâ”€> Apply LoRA for efficient training                                  â”‚\n",
    "â”‚     â””â”€> Optional: Quantization for memory savings                          â”‚\n",
    "â”‚                                                                            â”‚\n",
    "â”‚  ðŸ‹ï¸ 3. TRAINING                                                            â”‚\n",
    "â”‚     â”œâ”€> Set hyperparameters (LR, batch size, epochs)                      â”‚\n",
    "â”‚     â”œâ”€> Configure optimizer (AdamW) and scheduler (cosine)                â”‚\n",
    "â”‚     â”œâ”€> Train with gradient accumulation                                   â”‚\n",
    "â”‚     â””â”€> Monitor loss and save checkpoints                                  â”‚\n",
    "â”‚                                                                            â”‚\n",
    "â”‚  ðŸ“Š 4. EVALUATION                                                          â”‚\n",
    "â”‚     â”œâ”€> Calculate perplexity on validation set                            â”‚\n",
    "â”‚     â”œâ”€> Generate sample outputs                                            â”‚\n",
    "â”‚     â””â”€> Compare with baseline                                              â”‚\n",
    "â”‚                                                                            â”‚\n",
    "â”‚  ðŸš€ 5. DEPLOYMENT                                                          â”‚\n",
    "â”‚     â”œâ”€> Merge LoRA weights (optional)                                      â”‚\n",
    "â”‚     â”œâ”€> Export model                                                       â”‚\n",
    "â”‚     â””â”€> Set up inference pipeline                                          â”‚\n",
    "â”‚                                                                            â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Try with a larger model** - LLaMA 2 7B, Mistral 7B\n",
    "2. **Experiment with different datasets** - OpenOrca, Dolphin, WizardLM\n",
    "3. **Learn about DPO/RLHF** - Align model with human preferences\n",
    "4. **Explore quantization** - GGUF, AWQ, GPTQ for deployment\n",
    "5. **Deploy your model** - vLLM, TGI, Ollama\n",
    "\n",
    "## ðŸ“š Recommended Resources\n",
    "\n",
    "- **Papers**: \"Attention is All You Need\", \"LoRA\", \"QLoRA\", \"DPO\"\n",
    "- **Libraries**: Hugging Face, Unsloth (2x faster LoRA), Axolotl\n",
    "- **Datasets**: HuggingFace Hub (tatsu-lab/alpaca, OpenOrca)\n",
    "- **Compute**: Google Colab (free GPU), Lambda Labs, RunPod\n",
    "\n",
    "---\n",
    "*Built for aspiring LLM engineers ðŸš€*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
